{
  "paragraphs": [
    {
      "title": "Snippet 1 - zadanie",
      "text": "%spark\n\n/*\n     Waszym zadaniem jest odpowiednie zainicjalizowanie RDD wierzcholkow i krawedzi \n*/\n\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n\nval vertexArray \u003d Array(\n  (1L, (\"Alice\", 28)),\n  (2L, (\"Bob\", 27)),\n  (3L, (\"Charlie\", 65)),\n  (4L, (\"David\", 42)),\n  (5L, (\"Ed\", 55)),\n  (6L, (\"Fran\", 50))\n  )\nval edgeArray \u003d Array(\n  Edge(2L, 1L, 7),\n  Edge(2L, 4L, 2),\n  Edge(3L, 2L, 4),\n  Edge(3L, 6L, 3),\n  Edge(4L, 1L, 1),\n  Edge(5L, 2L, 2),\n  Edge(5L, 3L, 8),\n  Edge(5L, 6L, 3)\n  )\n  \n//val vertexRDD: RDD[(Long, (String, Int))] \u003d // Implement\n//val edgeRDD: RDD[Edge[Int]] \u003d // Implement\n",
      "dateUpdated": "Dec 4, 2016 8:36:05 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1480872016331_-47337552",
      "id": "20161204-172016_1368208492",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nimport org.apache.spark.graphx._\n\nimport org.apache.spark.rdd.RDD\n\nvertexArray: Array[(Long, (String, Int))] \u003d Array((1,(Alice,28)), (2,(Bob,27)), (3,(Charlie,65)), (4,(David,42)), (5,(Ed,55)), (6,(Fran,50)))\n\nedgeArray: Array[org.apache.spark.graphx.Edge[Int]] \u003d Array(Edge(2,1,7), Edge(2,4,2), Edge(3,2,4), Edge(3,6,3), Edge(4,1,1), Edge(5,2,2), Edge(5,3,8), Edge(5,6,3))\n\nvertexRDD: org.apache.spark.rdd.RDD[(Long, (String, Int))] \u003d ParallelCollectionRDD[21] at parallelize at \u003cconsole\u003e:45\n\nedgeRDD: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] \u003d ParallelCollectionRDD[22] at parallelize at \u003cconsole\u003e:45\n"
      },
      "dateCreated": "Dec 4, 2016 5:20:16 PM",
      "dateStarted": "Dec 4, 2016 7:17:31 PM",
      "dateFinished": "Dec 4, 2016 7:17:33 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Snippet 2 - zadanie",
      "text": "%spark\n\n/*\n     Waszym zadaniem jest wyswietlenie uzytkownikow ktorzy maja przynajmniej 30 lat\n*/\n\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n\nval graph: Graph[(String, Int), Int] \u003d Graph(vertexRDD, edgeRDD)\n\n\n//graph.vertices.filter {\n//  case (id, (name, age)) \u003d\u003e /* implement */\n//}.collect.foreach {\n//  case (id, (name, age)) \u003d\u003e /* implement */\n//}\n\n\n/*\nOczekiwany rezultat:\n\n    David is 42\n    Fran is 50\n    Ed is 55\n    Charlie is 65\n*/\n\n",
      "dateUpdated": "Dec 4, 2016 8:36:35 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1480872472551_-948958587",
      "id": "20161204-172752_794446119",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nimport org.apache.spark.graphx._\n\nimport org.apache.spark.rdd.RDD\n\ngraph: org.apache.spark.graphx.Graph[(String, Int),Int] \u003d org.apache.spark.graphx.impl.GraphImpl@66b47ae0\nCharlie is 65\nDavid is 42\nEd is 55\nFran is 50\n"
      },
      "dateCreated": "Dec 4, 2016 5:27:52 PM",
      "dateStarted": "Dec 4, 2016 7:17:41 PM",
      "dateFinished": "Dec 4, 2016 7:17:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Snippet 3 - zadanie",
      "text": "%spark\n\n/*\n    Waszym zadaniem jest wy≈õwietlenie opisu relacji \"kto lubi kogo\". Patrzcie -\u003e Oczekiwany rezultat\n*/\n\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n\n//for (triplet \u003c- graph.triplets.collect) {\n /**\n   * Triplet has the following Fields:\n   *   triplet.srcAttr: (String, Int) // triplet.srcAttr._1 is the name\n   *   triplet.dstAttr: (String, Int)\n   *   triplet.attr: Int\n   *   triplet.srcId: VertexId\n   *   triplet.dstId: VertexId\n   */\n//}\n\n/*\nOczekiwany rezultat:\n\n    Bob likes Alice\n    Bob likes David\n    Charlie likes Bob\n    Charlie likes Fran\n    David likes Alice\n    Ed likes Bob\n    Ed likes Charlie\n    Ed likes Fran\n*/\n",
      "dateUpdated": "Dec 4, 2016 8:39:35 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1480873196661_-838860750",
      "id": "20161204-173956_763685756",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nimport org.apache.spark.graphx._\n\nimport org.apache.spark.rdd.RDD\nBob likes Alice\nBob likes David\nCharlie likes Bob\nCharlie likes Fran\nDavid likes Alice\nEd likes Bob\nEd likes Charlie\nEd likes Fran\n"
      },
      "dateCreated": "Dec 4, 2016 5:39:56 PM",
      "dateStarted": "Dec 4, 2016 7:17:52 PM",
      "dateFinished": "Dec 4, 2016 7:17:53 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Snippet 4 - przyklad",
      "text": "%spark\n\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n\n// Define a class to more clearly model the user property\ncase class User(name: String, age: Int, inDeg: Int, outDeg: Int)\n// Create a user Graph\nval initialUserGraph: Graph[User, Int] \u003d graph.mapVertices{ case (id, (name, age)) \u003d\u003e User(name, age, 0, 0) }\n\n// Fill in the degree information\nval userGraph \u003d initialUserGraph.outerJoinVertices(initialUserGraph.inDegrees) {\n  case (id, u, inDegOpt) \u003d\u003e User(u.name, u.age, inDegOpt.getOrElse(0), u.outDeg)\n}.outerJoinVertices(initialUserGraph.outDegrees) {\n  case (id, u, outDegOpt) \u003d\u003e User(u.name, u.age, u.inDeg, outDegOpt.getOrElse(0))\n}\n\n",
      "dateUpdated": "Dec 4, 2016 8:41:37 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1480874709794_179634469",
      "id": "20161204-180509_978359032",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nimport org.apache.spark.graphx._\n\nimport org.apache.spark.rdd.RDD\n\ndefined class User\n\ninitialUserGraph: org.apache.spark.graphx.Graph[User,Int] \u003d org.apache.spark.graphx.impl.GraphImpl@4cffb7ee\n\nuserGraph: org.apache.spark.graphx.Graph[User,Int] \u003d org.apache.spark.graphx.impl.GraphImpl@51d9e2f9\nUser 1 is called Alice and is liked by 2 people.\nUser 2 is called Bob and is liked by 2 people.\nUser 3 is called Charlie and is liked by 1 people.\nUser 4 is called David and is liked by 1 people.\nUser 5 is called Ed and is liked by 0 people.\nUser 6 is called Fran and is liked by 2 people.\n"
      },
      "dateCreated": "Dec 4, 2016 6:05:09 PM",
      "dateStarted": "Dec 4, 2016 8:02:34 PM",
      "dateFinished": "Dec 4, 2016 8:02:36 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Snippet 5 - zadanie",
      "text": "%spark\n\n/*\n    Waszym zadaniem jest wypisanie liczby ludzi jaka lubi kazdego uzytkownika\n*/\n\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n\n\n\n/*\nOczekiwany rezultat:\n\n    User 1 is called Alice and is liked by 2 people.\n    User 2 is called Bob and is liked by 2 people.\n    User 3 is called Charlie and is liked by 1 people.\n    User 4 is called David and is liked by 1 people.\n    User 5 is called Ed and is liked by 0 people.\n    User 6 is called Fran and is liked by 2 people.\n*/\n",
      "dateUpdated": "Dec 4, 2016 8:43:48 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1480884087313_-1267234363",
      "id": "20161204-204127_1557898804",
      "dateCreated": "Dec 4, 2016 8:41:27 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Snippet 6 - zadanie",
      "text": "%spark\n\n/*\n    Waszym zadaniem jest wypisanie imion osob ktore sa lubiani przez taka sama liczbe osob jaka sami lubia\n*/\n\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n\n\n/*\nOczekiwany rezultat:\n\n    Bob\n    David\n*/\n\n",
      "dateUpdated": "Dec 4, 2016 8:46:51 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "lineNumbers": false,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1480880423177_1549703924",
      "id": "20161204-194023_631072155",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nimport org.apache.spark.graphx._\n\nimport org.apache.spark.rdd.RDD\nBob\nDavid\n"
      },
      "dateCreated": "Dec 4, 2016 7:40:23 PM",
      "dateStarted": "Dec 4, 2016 8:02:38 PM",
      "dateFinished": "Dec 4, 2016 8:02:39 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Snippet 7 - przyklad",
      "text": "%spark\n\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n\n// Find the oldest follower for each user\nval oldestFollower: VertexRDD[(String, Int)] \u003d userGraph.aggregateMessages[(String, Int)](\n  // For each edge send a message to the destination vertex with the attribute of the source vertex\n  edge \u003d\u003e edge.sendToDst(edge.srcAttr.name, edge.srcAttr.age),\n  // To combine messages take the message for the older follower\n  (a, b) \u003d\u003e if (a._2 \u003e b._2) a else b\n  )\n  \noldestFollower.collect.foreach {\n   case (id, str) \u003d\u003e println(str)\n}",
      "dateUpdated": "Dec 4, 2016 10:19:08 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1480884866061_-254272063",
      "id": "20161204-205426_379737290",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nimport org.apache.spark.graphx._\n\nimport org.apache.spark.rdd.RDD\n\noldestFollower: org.apache.spark.graphx.VertexRDD[(String, Int)] \u003d VertexRDDImpl[249] at RDD at VertexRDD.scala:57\n(David,42)\n(Charlie,65)\n(Ed,55)\n(Bob,27)\n(Charlie,65)\n"
      },
      "dateCreated": "Dec 4, 2016 8:54:26 PM",
      "dateStarted": "Dec 4, 2016 10:19:08 PM",
      "dateFinished": "Dec 4, 2016 10:19:09 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Snippet 8 - zadanie",
      "text": "%spark\n\n\n/*\n    Waszym zadaniem jest znalezienie dla kazdego usera imienia jego najstarszego followera\n*/\n\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n\n// Find the oldest follower for each user\nval oldestFollower: VertexRDD[(String, Int)] \u003d userGraph.aggregateMessages[(String, Int)](\n  // For each edge send a message to the destination vertex with the attribute of the source vertex\n  edge \u003d\u003e edge.sendToDst(edge.srcAttr.name, edge.srcAttr.age),\n  // To combine messages take the message for the older follower\n  (a, b) \u003d\u003e if (a._2 \u003e b._2) a else b\n  )\n\nuserGraph.vertices.leftJoin(oldestFollower) { (id, user, optOldestFollower) \u003d\u003e\n  /**\n   *\n   * Try using the match syntax:\n   *\n   *  optOldestFollower match {\n   *    case None \u003d\u003e \"No followers! implement me!\"\n   *    case Some((name, age)) \u003d\u003e \"implement me!\"\n   *  }\n   *\n   */\n}\n\n/*\n    David is the oldest follower of Alice.\n    Charlie is the oldest follower of Bob.\n    Ed is the oldest follower of Charlie.\n    Bob is the oldest follower of David.\n    Ed does not have any followers.\n    Charlie is the oldest follower of Fran.\n*/",
      "dateUpdated": "Dec 4, 2016 10:24:32 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1480885009730_1434607042",
      "id": "20161204-205649_700958540",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nimport org.apache.spark.graphx._\n\nimport org.apache.spark.rdd.RDD\n\noldestFollower: org.apache.spark.graphx.VertexRDD[(String, Int)] \u003d VertexRDDImpl[253] at RDD at VertexRDD.scala:57\nDavid is the oldest follower of Alice.\nCharlie is the oldest follower of Bob.\nEd is the oldest follower of Charlie.\nBob is the oldest follower of David.\nEd does not have any followers.\nCharlie is the oldest follower of Fran.\n"
      },
      "dateCreated": "Dec 4, 2016 8:56:49 PM",
      "dateStarted": "Dec 4, 2016 10:21:38 PM",
      "dateFinished": "Dec 4, 2016 10:21:39 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Snippet 9 - zadanie",
      "text": "%spark\n\n/*\n    Waszym zadaniem jest znalezienie sredniej wieku followerso\u0027w dla kazdego uzytkownika\n*/\n\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n\n\nval averageAge: VertexRDD[Double] \u003d userGraph.aggregateMessages[(Int, Double)](\n  // map function returns a tuple of (1, Age)\n  // put map function here,\n  // reduce function combines (sumOfFollowers, sumOfAge)\n  // put reduce function here\n  ).mapValues((id, p) \u003d\u003e p._2 / p._1)\n\n// Display the results\nuserGraph.vertices.leftJoin(averageAge) { (id, user, optAverageAge) \u003d\u003e\n  optAverageAge match {\n    case None \u003d\u003e s\"${user.name} does not have any followers.\"\n    case Some(avgAge) \u003d\u003e s\"The average age of ${user.name}\\\u0027s followers is $avgAge.\"\n  }\n}.collect.foreach { case (id, str) \u003d\u003e println(str) }\n\n/*\nOczekiwany rezultat:\n\nThe average age of Alice\u0027s followers is 34.5.\nThe average age of Bob\u0027s followers is 60.0.\nThe average age of Charlie\u0027s followers is 55.0.\nThe average age of David\u0027s followers is 27.0.\nEd does not have any followers.\nThe average age of Fran\u0027s followers is 60.0.\n\n*/\n",
      "dateUpdated": "Dec 4, 2016 9:44:52 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1480880585077_2053137731",
      "id": "20161204-194305_1917569452",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\nimport org.apache.spark.graphx._\n\nimport org.apache.spark.rdd.RDD\n\n\n\n\n\u003cconsole\u003e:131: error: not enough arguments for method aggregateMessages: (sendMsg: org.apache.spark.graphx.EdgeContext[User,Int,(Int, Double)] \u003d\u003e Unit, mergeMsg: ((Int, Double), (Int, Double)) \u003d\u003e (Int, Double), tripletFields: org.apache.spark.graphx.TripletFields)(implicit evidence$11: scala.reflect.ClassTag[(Int, Double)])org.apache.spark.graphx.VertexRDD[(Int, Double)].\nUnspecified value parameters sendMsg, mergeMsg.\n       val averageAge: VertexRDD[Double] \u003d userGraph.aggregateMessages[(Int, Double)](\n                                                                                     ^\n"
      },
      "dateCreated": "Dec 4, 2016 7:43:05 PM",
      "dateStarted": "Dec 4, 2016 9:44:36 PM",
      "dateFinished": "Dec 4, 2016 9:44:37 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Snippet 10 - zadanie",
      "text": "%spark\n\n/*\n    Waszym zadaniem jest jest wywolanie alogrytmu connectedComponents na grafie older graph\n    A nastepnie wypisaniu skladowych connected components\n*/\n\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n\nval olderGraph \u003d userGraph.subgraph(vpred \u003d (id, user) \u003d\u003e user.age \u003e\u003d 30)\n\n// compute the connected components graph here\n\n\n// display the component id of each user:\nolderGraph.vertices.leftJoin(cc.vertices) {\n  case (id, user, comp) \u003d\u003e s\"${user.name} is in component ${comp.get}\"\n}.collect.foreach{ case (id, str) \u003d\u003e println(str) }\n\n/*\nOczekiwany rezultat:\n\n    Charlie is in component 3\n    David is in component 4\n    Ed is in component 3\n    Fran is in component 3\n*/",
      "dateUpdated": "Dec 4, 2016 9:50:43 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1480880869202_1832108496",
      "id": "20161204-194749_1466466089",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nimport org.apache.spark.graphx._\n\nimport org.apache.spark.rdd.RDD\n\nolderGraph: org.apache.spark.graphx.Graph[User,Int] \u003d org.apache.spark.graphx.impl.GraphImpl@36443e87\n\ncc: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId,Int] \u003d org.apache.spark.graphx.impl.GraphImpl@2d627bc0\nCharlie is in component 3\nDavid is in component 4\nEd is in component 3\nFran is in component 3\n"
      },
      "dateCreated": "Dec 4, 2016 7:47:49 PM",
      "dateStarted": "Dec 4, 2016 8:26:19 PM",
      "dateFinished": "Dec 4, 2016 8:26:21 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Zadanie 2 (polecenia na slajdach)",
      "text": "%spark\n\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\nimport scala.util.MurmurHash\n\nval dataFrame \u003d sqlContext.read.options(Map(\"header\" -\u003e \"true\")).csv(\"data/2008.csv\")\n\n// zobaczmy na schemat wczytanych danych \ndataFrame.printSchema\n\n// zobaczmy na przykladowe dane\ndataFrame.take(2)\n\n// przygotujmy struktury potrzebne do stwoerzenia graphx\u0027owego grafu\n\nval flightsFromTo \u003d dataFrame.select($\"Origin\",$\"Dest\")\nval airportCodes \u003d dataFrame.select($\"Origin\", $\"Dest\").flatMap(x \u003d\u003e Iterable(x(0).toString, x(1).toString))\nval airportVertices: RDD[(VertexId, String)] \u003d airportCodes.distinct().map((x : String) \u003d\u003e ((MurmurHash.stringHash(x): Long), x)).rdd\nval defaultAirport \u003d (\"Missing\")\nval flightEdges \u003d flightsFromTo.map(x \u003d\u003e ((MurmurHash.stringHash(x(0).toString),MurmurHash.stringHash(x(1).toString)), 1)).rdd.reduceByKey(_+_).map(x \u003d\u003e Edge(x._1._1, x._1._2,x._2))\nval graph \u003d Graph(airportVertices, flightEdges, defaultAirport)\ngraph.persist() // we\u0027re going to be using it a lot\n\n// Basic statistic\n\n// Jak duzy jest nasz graf \ngraph.numVertices // 305\ngraph.numEdges // 5366\n\n// 10 najczestszych lotow z lotniska do lotniska\ngraph.triplets.sortBy(_.attr, ascending\u003dfalse).map(triplet \u003d\u003e \"There were \" + triplet.attr.toString + \" flights from \" + triplet.srcAttr + \" to \" + triplet.dstAttr + \".\").take(10)\n\n// 10 najrzadszych lotow z lotniska do lotniska\ngraph.triplets.sortBy(_.attr).map(triplet \u003d\u003e \"There were \" + triplet.attr.toString + \" flights from \" + triplet.srcAttr + \" to \" + triplet.dstAttr + \".\").take(10)\n\n// lotnisko o najwiekszej liczbie lotow przylatujacych\ngraph.inDegrees.join(airportVertices).sortBy(_._2._1, ascending\u003dfalse).take(1)\n\n// lotnisko o najwiekszej liczbie lotow odlatujacych\ngraph.outDegrees.join(airportVertices).sortBy(_._2._1, ascending\u003dfalse).take(1)\n\n// PageRank - tu czesc dla ludzi, korzystajac z utworzonych struktur prosze znalezc najbardziej popularne lotniska wg algorytmu Pange Rank\n\nval ranks \u003d graph.pageRank(0.0001).vertices\n\nval ranksAndAirports \u003d ranks.join(airportVertices).sortBy(_._2._1, ascending\u003dfalse).map(_._2._2).take(10)",
      "dateUpdated": "Dec 4, 2016 9:07:28 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1480783042178_1043734289",
      "id": "20161203-163722_48549069",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nimport org.apache.spark.graphx._\n\nimport org.apache.spark.rdd.RDD\n\nimport scala.util.MurmurHash\n\ndataFrame: org.apache.spark.sql.DataFrame \u003d [Year: string, Month: string ... 27 more fields]\nroot\n |-- Year: string (nullable \u003d true)\n |-- Month: string (nullable \u003d true)\n |-- DayofMonth: string (nullable \u003d true)\n |-- DayOfWeek: string (nullable \u003d true)\n |-- DepTime: string (nullable \u003d true)\n |-- CRSDepTime: string (nullable \u003d true)\n |-- ArrTime: string (nullable \u003d true)\n |-- CRSArrTime: string (nullable \u003d true)\n |-- UniqueCarrier: string (nullable \u003d true)\n |-- FlightNum: string (nullable \u003d true)\n |-- TailNum: string (nullable \u003d true)\n |-- ActualElapsedTime: string (nullable \u003d true)\n |-- CRSElapsedTime: string (nullable \u003d true)\n |-- AirTime: string (nullable \u003d true)\n |-- ArrDelay: string (nullable \u003d true)\n |-- DepDelay: string (nullable \u003d true)\n |-- Origin: string (nullable \u003d true)\n |-- Dest: string (nullable \u003d true)\n |-- Distance: string (nullable \u003d true)\n |-- TaxiIn: string (nullable \u003d true)\n |-- TaxiOut: string (nullable \u003d true)\n |-- Cancelled: string (nullable \u003d true)\n |-- CancellationCode: string (nullable \u003d true)\n |-- Diverted: string (nullable \u003d true)\n |-- CarrierDelay: string (nullable \u003d true)\n |-- WeatherDelay: string (nullable \u003d true)\n |-- NASDelay: string (nullable \u003d true)\n |-- SecurityDelay: string (nullable \u003d true)\n |-- LateAircraftDelay: string (nullable \u003d true)\n\n\nres46: Array[org.apache.spark.sql.Row] \u003d Array([2008,1,3,4,2003,1955,2211,2225,WN,335,N712SW,128,150,116,-14,8,IAD,TPA,810,4,8,0,null,0,NA,NA,NA,NA,NA], [2008,1,3,4,754,735,1002,1000,WN,3231,N772SW,128,145,113,2,19,IAD,TPA,810,5,10,0,null,0,NA,NA,NA,NA,NA])\n\nflightsFromTo: org.apache.spark.sql.DataFrame \u003d [Origin: string, Dest: string]\n\nairportCodes: org.apache.spark.sql.Dataset[String] \u003d [value: string]\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nairportVertices: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, String)] \u003d MapPartitionsRDD[1984] at rdd at \u003cconsole\u003e:161\n\ndefaultAirport: String \u003d Missing\n\nwarning: there were two deprecation warnings; re-run with -deprecation for details\n\nflightEdges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] \u003d MapPartitionsRDD[1991] at map at \u003cconsole\u003e:161\n\ngraph: org.apache.spark.graphx.Graph[String,Int] \u003d org.apache.spark.graphx.impl.GraphImpl@8b851c2\n\nres47: org.apache.spark.graphx.Graph[String,Int] \u003d org.apache.spark.graphx.impl.GraphImpl@8b851c2\n\nres48: Long \u003d 305\n\nres49: Long \u003d 5366\n\nres50: Array[String] \u003d Array(There were 13788 flights from SFO to LAX., There were 13390 flights from LAX to SFO., There were 12383 flights from OGG to HNL., There were 12035 flights from LGA to BOS., There were 12029 flights from BOS to LGA., There were 12014 flights from HNL to OGG., There were 11773 flights from LAX to LAS., There were 11729 flights from LAS to LAX., There were 11257 flights from LAX to SAN., There were 11224 flights from SAN to LAX.)\n\nres51: Array[String] \u003d Array(There were 1 flights from PDX to MSN., There were 1 flights from BOS to GGG., There were 1 flights from DEN to ROC., There were 1 flights from DEN to CYS., There were 1 flights from PHL to GRR., There were 1 flights from PHL to ICT., There were 1 flights from FSD to MSN., There were 1 flights from FSD to SDF., There were 1 flights from BTV to IND., There were 1 flights from TUL to OMA.)\n\nres52: Array[(org.apache.spark.graphx.VertexId, (Int, String))] \u003d Array((2042033420,(173,ATL)))\n\nres53: Array[(org.apache.spark.graphx.VertexId, (Int, String))] \u003d Array((2042033420,(173,ATL)))\n\nranks: org.apache.spark.graphx.VertexRDD[Double] \u003d VertexRDDImpl[2827] at RDD at VertexRDD.scala:57\n\nranksAndAirports: Array[String] \u003d Array(ATL, DFW, ORD, MSP, SLC, DEN, DTW, IAH, CVG, LAX)\n"
      },
      "dateCreated": "Dec 3, 2016 4:37:22 PM",
      "dateStarted": "Dec 3, 2016 10:23:51 PM",
      "dateFinished": "Dec 3, 2016 10:25:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Zadanie 2",
      "text": "",
      "dateUpdated": "Dec 4, 2016 9:08:18 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1480783011851_1837289024",
      "id": "20161203-163651_570312931",
      "dateCreated": "Dec 3, 2016 4:36:51 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Apache Spark - GraphX",
  "id": "2C4QDEKRU",
  "angularObjects": {
    "2C3K545SF:shared_process": [],
    "2C3VD16JY:shared_process": [],
    "2C3RV46CN:shared_process": [],
    "2C2MW31MG:shared_process": [],
    "2C43SX4GF:shared_process": [],
    "2C36RHUW7:shared_process": [],
    "2C59WSV73:shared_process": [],
    "2C2EPCTC7:shared_process": [],
    "2C2Z45BK5:shared_process": []
  },
  "config": {},
  "info": {}
}